{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA1MetB43GlF"
      },
      "source": [
        "## Install Libraries & Prepare the Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NwSra4MgFDq",
        "outputId": "ff0cd062-10c5-4bf6-d5ec-9025aced249b"
      },
      "outputs": [],
      "source": [
        "# %pip install scikit-optimize\n",
        "# %pip install boto3\n",
        "# %pip install dgl==0.9.0\n",
        "# %pip install dgl-cu101\n",
        "\n",
        "from torch.multiprocessing import Pool, Process, set_start_method\n",
        "import dgl.function as dgl_func\n",
        "import dgl\n",
        "import torch\n",
        "import dgl.function as fn\n",
        "import dgl.nn.pytorch as dglnn\n",
        "from dgl.dataloading import MultiLayerNeighborSampler, as_edge_prediction_sampler\n",
        "import pandas as pd\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as torch_nn_func\n",
        "import dgl\n",
        "import dgl.nn.pytorch as dglnn\n",
        "import dgl.function as dgl_func\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "from collections import defaultdict\n",
        "from typing import Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from datetime import datetime\n",
        "import textwrap\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzobtrqgck9E"
      },
      "source": [
        "## Đọc dữ liệu user, product, interact từ csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "phElQR3JsJVt",
        "outputId": "dff2a3f7-e499-4330-8921-2f6ed311e4bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6040, 4)\n",
            "(33, 12)\n"
          ]
        }
      ],
      "source": [
        "user_feat_df = pd.read_csv('./collect_data/user.csv')\n",
        "interact_df = pd.read_csv('./collect_data/rating.csv')\n",
        "product_feat_df = pd.read_csv('./collect_data/product.csv')\n",
        "interact_df.sort_values(by='Timestamp')\n",
        "interact_df.head()\n",
        "print(user_feat_df.shape)\n",
        "print(product_feat_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duplicate_rows = interact_df.duplicated(subset=['UserID', 'ItemID'], keep=False)\n",
        "num_duplicates = duplicate_rows.sum()\n",
        "num_duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U00006040</td>\n",
              "      <td>P0017</td>\n",
              "      <td>3</td>\n",
              "      <td>956704584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U00006037</td>\n",
              "      <td>P0017</td>\n",
              "      <td>4</td>\n",
              "      <td>956709701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>U00006036</td>\n",
              "      <td>P0024</td>\n",
              "      <td>2</td>\n",
              "      <td>956710067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U00006036</td>\n",
              "      <td>P0028</td>\n",
              "      <td>4</td>\n",
              "      <td>956710566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U00006035</td>\n",
              "      <td>P0025</td>\n",
              "      <td>1</td>\n",
              "      <td>956710846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15924</th>\n",
              "      <td>U00003391</td>\n",
              "      <td>P0005</td>\n",
              "      <td>2</td>\n",
              "      <td>1046188297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15925</th>\n",
              "      <td>U00003391</td>\n",
              "      <td>P0003</td>\n",
              "      <td>3</td>\n",
              "      <td>1046188641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15926</th>\n",
              "      <td>U00003391</td>\n",
              "      <td>P0025</td>\n",
              "      <td>3</td>\n",
              "      <td>1046189237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15927</th>\n",
              "      <td>U00003391</td>\n",
              "      <td>P0026</td>\n",
              "      <td>4</td>\n",
              "      <td>1046189862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15928</th>\n",
              "      <td>U00003391</td>\n",
              "      <td>P0007</td>\n",
              "      <td>3</td>\n",
              "      <td>1046191882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15929 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          UserID ItemID  Rating   Timestamp\n",
              "0      U00006040  P0017       3   956704584\n",
              "1      U00006037  P0017       4   956709701\n",
              "2      U00006036  P0024       2   956710067\n",
              "3      U00006036  P0028       4   956710566\n",
              "4      U00006035  P0025       1   956710846\n",
              "...          ...    ...     ...         ...\n",
              "15924  U00003391  P0005       2  1046188297\n",
              "15925  U00003391  P0003       3  1046188641\n",
              "15926  U00003391  P0025       3  1046189237\n",
              "15927  U00003391  P0026       4  1046189862\n",
              "15928  U00003391  P0007       3  1046191882\n",
              "\n",
              "[15929 rows x 4 columns]"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interact_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2270, 4)"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "interact_df[interact_df['Rating'] < 3].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T81TYy8UdDUD"
      },
      "source": [
        "## Chia tập train và test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "KG1okk1DchHI"
      },
      "outputs": [],
      "source": [
        "test_size = 0.28\n",
        "split_idx = int(len(interact_df) * (1 - test_size))\n",
        "rating_train_df = interact_df[: split_idx]\n",
        "rating_test_df = interact_df[split_idx: ]\n",
        "rating_all_df = interact_df[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      UserID ItemID  Rating  Timestamp\n",
            "0  U00006040  P0017       3  956704584\n",
            "1  U00006037  P0017       4  956709701\n",
            "2  U00006036  P0024       2  956710067\n",
            "3  U00006036  P0028       4  956710566\n",
            "4  U00006035  P0025       1  956710846\n",
            "(15929, 4)\n",
            "(11468, 4)\n",
            "(4461, 4)\n",
            "(15929, 4)\n"
          ]
        }
      ],
      "source": [
        "print(rating_train_df.head())\n",
        "print(interact_df.shape)\n",
        "print(rating_train_df.shape)\n",
        "print(rating_test_df.shape)\n",
        "print(rating_all_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15929, 4)"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_all_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvu40rj0d0av"
      },
      "source": [
        "## Xử lý và đặt id node cho user trên tập train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "0KmxGDVisYY7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3334, 2)"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_id_train_df = pd.DataFrame(rating_train_df['UserID'].unique(), columns=['UserID'])\n",
        "user_id_train_df.shape\n",
        "user_id_train_df['user_new_id'] = user_id_train_df.index\n",
        "user_id_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id_table = user_id_train_df[['UserID', 'user_new_id']]\n",
        "user_id_table.to_csv('user_id_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nslcgqgmfBB8"
      },
      "source": [
        "## Xử lý và đặt id node cho sản phẩm (lấy tất cả sản phẩm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_product = rating_train_df['ItemID'].unique().tolist()\n",
        "all_product = product_feat_df['ItemID'].unique().tolist()\n",
        "\n",
        "unbuy_product = [product for product in all_product if product not in train_product]\n",
        "train_product.extend(unbuy_product)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U00004506</td>\n",
              "      <td>P0001</td>\n",
              "      <td>5</td>\n",
              "      <td>964981763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U00005557</td>\n",
              "      <td>P0001</td>\n",
              "      <td>5</td>\n",
              "      <td>959440917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>U00005206</td>\n",
              "      <td>P0001</td>\n",
              "      <td>4</td>\n",
              "      <td>961629983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U00001552</td>\n",
              "      <td>P0001</td>\n",
              "      <td>4</td>\n",
              "      <td>974742562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U00002849</td>\n",
              "      <td>P0001</td>\n",
              "      <td>4</td>\n",
              "      <td>972509670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11463</th>\n",
              "      <td>U00003847</td>\n",
              "      <td>P0032</td>\n",
              "      <td>4</td>\n",
              "      <td>965880572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11464</th>\n",
              "      <td>U00003850</td>\n",
              "      <td>P0032</td>\n",
              "      <td>5</td>\n",
              "      <td>965880197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11465</th>\n",
              "      <td>U00004562</td>\n",
              "      <td>P0032</td>\n",
              "      <td>4</td>\n",
              "      <td>967430875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11466</th>\n",
              "      <td>U00003650</td>\n",
              "      <td>P0033</td>\n",
              "      <td>3</td>\n",
              "      <td>966460690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11467</th>\n",
              "      <td>U00003377</td>\n",
              "      <td>P0033</td>\n",
              "      <td>3</td>\n",
              "      <td>967587131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11468 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          UserID ItemID  Rating  Timestamp\n",
              "0      U00004506  P0001       5  964981763\n",
              "1      U00005557  P0001       5  959440917\n",
              "2      U00005206  P0001       4  961629983\n",
              "3      U00001552  P0001       4  974742562\n",
              "4      U00002849  P0001       4  972509670\n",
              "...          ...    ...     ...        ...\n",
              "11463  U00003847  P0032       4  965880572\n",
              "11464  U00003850  P0032       5  965880197\n",
              "11465  U00004562  P0032       4  967430875\n",
              "11466  U00003650  P0033       3  966460690\n",
              "11467  U00003377  P0033       3  967587131\n",
              "\n",
              "[11468 rows x 4 columns]"
            ]
          },
          "execution_count": 212,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_train_df.sort_values(by='ItemID', ignore_index=True, inplace=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "r2sbuCXEekbz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(33, 2)"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "product_id_all_df = pd.DataFrame(train_product, columns=['ItemID'])\n",
        "product_id_all_df['product_new_id'] = product_id_all_df.index\n",
        "product_id_all_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>U00004506</td>\n",
              "      <td>P0001</td>\n",
              "      <td>5</td>\n",
              "      <td>964981763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U00005557</td>\n",
              "      <td>P0001</td>\n",
              "      <td>5</td>\n",
              "      <td>959440917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>U00005206</td>\n",
              "      <td>P0001</td>\n",
              "      <td>4</td>\n",
              "      <td>961629983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U00001552</td>\n",
              "      <td>P0001</td>\n",
              "      <td>4</td>\n",
              "      <td>974742562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>U00002849</td>\n",
              "      <td>P0001</td>\n",
              "      <td>4</td>\n",
              "      <td>972509670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11463</th>\n",
              "      <td>U00003847</td>\n",
              "      <td>P0032</td>\n",
              "      <td>4</td>\n",
              "      <td>965880572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11464</th>\n",
              "      <td>U00003850</td>\n",
              "      <td>P0032</td>\n",
              "      <td>5</td>\n",
              "      <td>965880197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11465</th>\n",
              "      <td>U00004562</td>\n",
              "      <td>P0032</td>\n",
              "      <td>4</td>\n",
              "      <td>967430875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11466</th>\n",
              "      <td>U00003650</td>\n",
              "      <td>P0033</td>\n",
              "      <td>3</td>\n",
              "      <td>966460690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11467</th>\n",
              "      <td>U00003377</td>\n",
              "      <td>P0033</td>\n",
              "      <td>3</td>\n",
              "      <td>967587131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11468 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          UserID ItemID  Rating  Timestamp\n",
              "0      U00004506  P0001       5  964981763\n",
              "1      U00005557  P0001       5  959440917\n",
              "2      U00005206  P0001       4  961629983\n",
              "3      U00001552  P0001       4  974742562\n",
              "4      U00002849  P0001       4  972509670\n",
              "...          ...    ...     ...        ...\n",
              "11463  U00003847  P0032       4  965880572\n",
              "11464  U00003850  P0032       5  965880197\n",
              "11465  U00004562  P0032       4  967430875\n",
              "11466  U00003650  P0033       3  966460690\n",
              "11467  U00003377  P0033       3  967587131\n",
              "\n",
              "[11468 rows x 4 columns]"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_train_df.sort_values(by='ItemID', ignore_index=True, inplace=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ItemID</th>\n",
              "      <th>product_new_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P0001</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P0002</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P0003</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P0004</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P0005</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>P0006</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>P0007</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>P0008</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>P0009</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>P0010</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>P0011</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>P0012</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>P0013</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>P0014</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>P0015</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>P0016</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>P0017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>P0018</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>P0019</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>P0020</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>P0021</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>P0022</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>P0023</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>P0024</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>P0025</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>P0026</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>P0027</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>P0028</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>P0029</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>P0030</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>P0031</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>P0032</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>P0033</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ItemID  product_new_id\n",
              "0   P0001              11\n",
              "1   P0002              18\n",
              "2   P0003              14\n",
              "3   P0004               6\n",
              "4   P0005              13\n",
              "5   P0006              15\n",
              "6   P0007              12\n",
              "7   P0008              16\n",
              "8   P0009              28\n",
              "9   P0010              19\n",
              "10  P0011               5\n",
              "11  P0012              27\n",
              "12  P0013              26\n",
              "13  P0014              25\n",
              "14  P0015              20\n",
              "15  P0016              22\n",
              "16  P0017               0\n",
              "17  P0018              30\n",
              "18  P0019              23\n",
              "19  P0020              24\n",
              "20  P0021               7\n",
              "21  P0022              21\n",
              "22  P0023              29\n",
              "23  P0024               1\n",
              "24  P0025               3\n",
              "25  P0026               8\n",
              "26  P0027              31\n",
              "27  P0028               2\n",
              "28  P0029              17\n",
              "29  P0030              10\n",
              "30  P0031               9\n",
              "31  P0032               4\n",
              "32  P0033              32"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "product_id_all_df.sort_values(by='ItemID', ignore_index=True, inplace=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "product_id_table = product_id_all_df[['ItemID', 'product_new_id']]\n",
        "product_id_table.to_csv('product_id_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpEMJ6mshAgD"
      },
      "source": [
        "## Xử lý df rating trên tập train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "Bhn5TLKJhIt_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11468, 6)"
            ]
          },
          "execution_count": 217,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_train_df = rating_train_df.merge(user_id_train_df, how='left', on='UserID')\n",
        "rating_train_df = rating_train_df.merge(product_id_all_df, how='left', on='ItemID')\n",
        "\n",
        "rating_train_df.drop_duplicates(subset=['Rating', 'user_new_id', 'product_new_id'], keep='last', inplace=True)\n",
        "rating_train_df.sort_values(by=['Rating', 'user_new_id', 'product_new_id'], ignore_index=True, inplace=True) \n",
        "rating_train_df.sort_values(by='Timestamp', ignore_index=True, inplace=True)  \n",
        "rating_train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOTuSILjKlR"
      },
      "source": [
        "## Xử lý df rating trên tập test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "keE8gPtjjNBL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1045,)"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_test_df = rating_test_df.merge(user_id_train_df, how='inner', on='UserID')\n",
        "rating_test_df = rating_test_df.merge(product_id_all_df, how='inner', on='ItemID')\n",
        "user_id_rating_test_src_df = rating_test_df['user_new_id'].values\n",
        "product_id_rating_test_dst_df = rating_test_df['product_new_id'].values\n",
        "ground_truth_test = (user_id_rating_test_src_df, product_id_rating_test_dst_df)\n",
        "user_id_rating_test_src_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>user_new_id</th>\n",
              "      <th>product_new_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [UserID, ItemID, Rating, Timestamp, user_new_id, product_new_id]\n",
              "Index: []"
            ]
          },
          "execution_count": 219,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_train_df[(rating_train_df['UserID'] == 'U00002389') & (rating_train_df['Rating'] == 1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>ItemID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>user_new_id</th>\n",
              "      <th>product_new_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [UserID, ItemID, Rating, Timestamp, user_new_id, product_new_id]\n",
              "Index: []"
            ]
          },
          "execution_count": 220,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_test_df[(rating_test_df['UserID'] == 'U00002389') & (rating_test_df['Rating'] == 1)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBGroz-7feu2"
      },
      "source": [
        "## Tạo ma trận liền kề trên tập train\n",
        "- src: các id node user\n",
        "- dst: các id node product\n",
        "- w: điểm rating của user cho product\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11468, 6)"
            ]
          },
          "execution_count": 221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "sTGiP1ugsNJ1"
      },
      "outputs": [],
      "source": [
        "adjacency_dict = {}\n",
        "\n",
        "adjacency_dict['user_product_rating'] = rating_train_df.Rating.values\n",
        "adjacency_dict['user_product_src'] = rating_train_df.user_new_id.values\n",
        "adjacency_dict['user_product_dst'] = rating_train_df.product_new_id.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(adjacency_dict['user_product_dst']).size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrdCzK_kiwIV"
      },
      "source": [
        "## Tạo graph Heterogeneous để train\n",
        "- 2 loại cạnh: rating và bought-by\n",
        "- 2 loại node: user và node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "wZl5s8VtsfZo"
      },
      "outputs": [],
      "source": [
        "graph_schema = {('user', 'rating', 'product'): list(zip(adjacency_dict['user_product_src'], adjacency_dict['user_product_dst'])),\n",
        "                ('product', 'bought-by', 'user'): list(zip(adjacency_dict['user_product_dst'], adjacency_dict['user_product_src']))\n",
        "               }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11468"
            ]
          },
          "execution_count": 225,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list(zip(adjacency_dict['user_product_src'], adjacency_dict['user_product_dst'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpCCCsBnslnQ",
        "outputId": "23a635e7-6fb8-4cc5-cc6f-5a360a5dc818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph --> \n",
            " Graph(num_nodes={'product': 33, 'user': 3334},\n",
            "      num_edges={('product', 'bought-by', 'user'): 11468, ('user', 'rating', 'product'): 11468},\n",
            "      metagraph=[('product', 'user', 'bought-by'), ('user', 'product', 'rating')])\n"
          ]
        }
      ],
      "source": [
        "hetero_graph = dgl.heterograph(graph_schema)\n",
        "print('Graph --> \\n', hetero_graph) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aknhoqo_kpT6"
      },
      "source": [
        "## Rút trích đặc tính của user train\n",
        "- Hiện tại chỉ có giới tính"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6040, 4)"
            ]
          },
          "execution_count": 227,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_feat_df.head()\n",
        "user_feat_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3334, 2)"
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_id_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "78co_AJr7TDm"
      },
      "outputs": [],
      "source": [
        "# Join với user được training để lấy những feature của các user đó và bỏ các user còn lại -> có 3664 user được training\n",
        "user_feat_train_df = user_feat_df.merge(user_id_train_df, how='inner', on='UserID')\n",
        "ids = user_feat_train_df.user_new_id.values.astype(int)\n",
        "feats = np.stack((user_feat_train_df.F.values, user_feat_train_df.M.values), axis=1)\n",
        "\n",
        "user_feat = np.zeros((hetero_graph.number_of_nodes('user'), 2))\n",
        "user_feat[ids] = feats\n",
        "\n",
        "user_feat = torch.tensor(user_feat).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3334, 2])"
            ]
          },
          "execution_count": 230,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3334, 5)"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_feat_train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbK6oR42k_YB"
      },
      "source": [
        "## Rút trích đặc tính của product\n",
        "- 10 hương vị"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(33, 12)\n",
            "(33, 2)\n"
          ]
        }
      ],
      "source": [
        "print(product_feat_df.shape)\n",
        "print(product_id_all_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "Lm-MvsX3lDMB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([33, 10])"
            ]
          },
          "execution_count": 233,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# có 33 sp và cả 33 được đi training \n",
        "product_feat_df = product_feat_df.merge(product_id_all_df, how='left', on='ItemID')\n",
        "product_feat_df = product_feat_df[product_feat_df.product_new_id < hetero_graph.number_of_nodes('product')]  \n",
        "\n",
        "ids = product_feat_df.product_new_id.values.astype(int)\n",
        "\n",
        "feats = np.stack((product_feat_df.Type.values,\n",
        "                  product_feat_df.Coffee.values,\n",
        "                  product_feat_df.Tea.values,\n",
        "                  product_feat_df.Chocolate.values,\n",
        "                  product_feat_df.Matcha.values,\n",
        "                  product_feat_df.Cream.values,\n",
        "                  product_feat_df.Vani.values,\n",
        "                  product_feat_df.Milk.values,\n",
        "                  product_feat_df.Pumpkin.values,\n",
        "                  product_feat_df.Cheese.values,\n",
        "                  ),\n",
        "                 axis=1)\n",
        "\n",
        "product_feat = np.zeros((hetero_graph.number_of_nodes('product'), feats.shape[1]))\n",
        "product_feat[ids] = feats\n",
        "product_feat = torch.tensor(product_feat).float()\n",
        "product_feat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Thêm các giá trị vào node và cạnh cho graph train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC0PVo3Smbwi"
      },
      "source": [
        "### Thêm các giá trị vào 2 node user và product "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_dict = {}\n",
        "features_dict['user_feat'] = user_feat\n",
        "features_dict['product_feat'] = product_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "XKRx2pMaWXXD"
      },
      "outputs": [],
      "source": [
        "hetero_graph.nodes['user'].data['features'] = features_dict['user_feat']\n",
        "hetero_graph.nodes['product'].data['features'] = features_dict['product_feat']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Thêm các giá trị vào cạnh rating và bought-by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [],
      "source": [
        "hetero_graph.edges['rating'].data['rating'] = torch.tensor(adjacency_dict['user_product_rating'])\n",
        "hetero_graph.edges['bought-by'].data['rating'] = torch.tensor(adjacency_dict['user_product_rating'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3, 4, 2,  ..., 1, 4, 4])"
            ]
          },
          "execution_count": 237,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor(adjacency_dict['user_product_rating'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwMpISuJonLT"
      },
      "source": [
        "## Node Embeddings\n",
        "\n",
        "* We calculate Node embeddings to represent nodes as vectors an capture the topology of the network\n",
        "* The embeddings --> based on similarity.\n",
        "* We will use the embeddings for prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "D51TSxay63lD"
      },
      "outputs": [],
      "source": [
        "class NodeEmbedding(nn.Module):\n",
        "    def __init__(self, input_features, output_features,):\n",
        "        super().__init__()\n",
        "        self.project_features = nn.Linear(input_features, output_features)\n",
        "\n",
        "    def forward(self, node_features):\n",
        "        return self.project_features(node_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zICu9p7NqX_3"
      },
      "source": [
        "## Message passing layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "OBf_17YCeULF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MessagePassing(nn.Module):\n",
        "    \n",
        "      def __init__(self, input_features, output_features, dropout,):\n",
        "        super().__init__()\n",
        "        self._in_neigh_feats, self._in_self_feats = input_features\n",
        "        self._output_features = output_features \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_self = nn.Linear(self._in_self_feats, output_features, bias=False)\n",
        "        self.fc_neighbour = nn.Linear(self._in_neigh_feats, output_features, bias=False)\n",
        "        self.fc_pre_agg = nn.Linear(self._in_neigh_feats, self._in_neigh_feats, bias=False)\n",
        "      \n",
        "      def forward(self, graph, x):\n",
        "        \n",
        "        h_neighbor, h_self = x\n",
        "        h_self = self.dropout(h_self)\n",
        "        h_neighbor = self.dropout(h_neighbor)\n",
        "        \n",
        "\n",
        "        graph.srcdata['h'] = torch_nn_func.relu(self.fc_pre_agg(h_neighbor))\n",
        "        graph.update_all(dgl_func.copy_u('h', 'm'), dgl_func.mean('m', 'neigh'))\n",
        "        h_neighbor = graph.dstdata['neigh']\n",
        "\n",
        "        #message passing\n",
        "        z = self.fc_self(h_self) + self.fc_neighbour(h_neighbor)\n",
        "        z = torch_nn_func.relu(z)\n",
        "\n",
        "        z_normalization = z.norm(2, 1, keepdim=True)\n",
        "        z_normalization = torch.where(z_normalization == 0, torch.tensor(1.).to(z_normalization), z_normalization)\n",
        "        z = z / z_normalization\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf5aU6sFt2FX"
      },
      "source": [
        "## Calculate Similarity between Users & Products\n",
        "\n",
        "Ref: https://docs.dgl.ai/en/0.6.x/guide/training-edge.html#guide-training-edge-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEKN6jUAuVhL"
      },
      "source": [
        "### Prediction for NN Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "MOIrBbKE66MJ"
      },
      "outputs": [],
      "source": [
        "class NnSimilarityPredictingLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_layer_dim: int):\n",
        "        super(NnSimilarityPredictingLayer, self).__init__()\n",
        "        self.hidden_layer_1 = nn.Linear(2 * embedding_layer_dim, hidden_layer_1_output_dim)\n",
        "        self.hidden_layer_2 = nn.Linear(hidden_layer_1_output_dim, hidden_layer_2_output_dim)\n",
        "        self.output = nn.Linear(hidden_layer_2_output_dim, 1)\n",
        "        self.relu_layer = nn.ReLU()\n",
        "        self.sigmoid_layer = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layer_1(x)\n",
        "        x = self.relu_layer(x)\n",
        "        x = self.hidden_layer_2(x)\n",
        "        x = self.relu_layer(x)\n",
        "        x = self.output(x)\n",
        "        x = self.sigmoid_layer(x)\n",
        "        return x\n",
        "\n",
        "class NnPredictingModule(nn.Module):\n",
        "    def __init__(self, predicting_layer, embed_dim: int):\n",
        "        super(NnPredictingModule, self).__init__()\n",
        "        self.layer_nn = NnSimilarityPredictingLayer(embed_dim)\n",
        "\n",
        "    def forward(self, graph, h ):\n",
        "        ratings_dict = {}\n",
        "        edge_types_list = ['user', 'product']\n",
        "\n",
        "        for edge_type in graph.canonical_etypes:\n",
        "            if edge_type[0] in edge_types_list and edge_type[2] in edge_types_list:\n",
        "                u_type, _, v_type = edge_type\n",
        "                src_nid, dst_nid = graph.all_edges(etype=edge_type) \n",
        "                emb_heads = h[u_type][src_nid]\n",
        "                emb_tails = h[v_type][dst_nid]\n",
        "                cat_embed = torch.cat((emb_heads, emb_tails), 1)\n",
        "                ratings = self.layer_nn(cat_embed)\n",
        "                ratings_dict[edge_type] = torch.flatten(ratings)\n",
        "        \n",
        "        ratings_dict = {key: torch.unsqueeze(ratings_dict[key], 1) for key in ratings_dict.keys()}\n",
        "\n",
        "        return ratings_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goUkXwQ3wYmb"
      },
      "source": [
        "### Prediction for Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "pYRyf5ji-DpP"
      },
      "outputs": [],
      "source": [
        "class CosinePrediction(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, h):\n",
        "        with graph.local_scope():\n",
        "            for edge_type in graph.canonical_etypes:\n",
        "                try:\n",
        "                    graph.nodes[edge_type[0]].data['norm_h'] = torch_nn_func.normalize(h[edge_type[0]], p=2, dim=-1)\n",
        "                    graph.nodes[edge_type[2]].data['norm_h'] = torch_nn_func.normalize(h[edge_type[2]], p=2, dim=-1)\n",
        "                    graph.apply_edges(fn.u_dot_v('norm_h', 'norm_h', 'cos'), etype=edge_type)\n",
        "                except ValueError:\n",
        "                   print(\"Cosine similarity fucntion is not correct!\")\n",
        "            ratings = graph.edata['cos']\n",
        "        return ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nket1fgyz7b0"
      },
      "source": [
        "### Prediction for Dot-Product Similarity\n",
        "\n",
        "Refs:\n",
        "- https://docs.dgl.ai/en/0.6.x/guide/minibatch-link.html\n",
        "- https://issueexplorer.com/issue/dmlc/dgl/3447"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "8vdpsUcABkp8"
      },
      "outputs": [],
      "source": [
        "class DotProductPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "    def forward(self, graph, h):\n",
        "      \n",
        "      with graph.local_scope():\n",
        "        for edge_type in graph.canonical_edge_types:\n",
        "          try:\n",
        "            graph.n_data['h'] = h\n",
        "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'), etype=edge_type)\n",
        "          except KeyError:\n",
        "            pass\n",
        "        ratings = graph.edge_data['score']\n",
        "\n",
        "      return ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5eCXAl7RKzz"
      },
      "source": [
        "### Prediction for PairWise Distance Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "KWtmvC9ptFOg"
      },
      "outputs": [],
      "source": [
        "class PairWiseDistancePredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, graph, h):\n",
        "\n",
        "        with graph.local_scope():\n",
        "            for edge_type in graph.canonical_edge_types:\n",
        "                try:\n",
        "                    graph.nodes[edge_type[0]].data['h'] = h[edge_type[0]]\n",
        "                    graph.nodes[edge_type[2]].data['h'] = h[edge_type[2]]\n",
        "                    graph.apply_edges(lambda edges: {'pwdist': nn.PairwiseDistance('h', 'h')}, etype=edge_type )\n",
        "                except KeyError:\n",
        "                    pass\n",
        "            ratings = graph.edge_data['pwdist']\n",
        "\n",
        "        return ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky4oPq-PQyRn"
      },
      "source": [
        "## GNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "9hqSDUQK_OWP"
      },
      "outputs": [],
      "source": [
        "class GNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, g, n_layers: int, dim_dict, dropout, pred, aggregator_hetero, embedding_layer,):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embedding_layer = embedding_layer\n",
        "\n",
        "        if embedding_layer:\n",
        "            self.user_embed = NodeEmbedding(dim_dict['user'], dim_dict['hidden'])\n",
        "            self.item_embed = NodeEmbedding(dim_dict['product'], dim_dict['hidden'])\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # input layer\n",
        "        if not embedding_layer:\n",
        "            self.layers.append(\n",
        "                dglnn.HeteroGraphConv(\n",
        "                    {etype[1]: MessagePassing((dim_dict[etype[0]], dim_dict[etype[2]]), dim_dict['hidden'], dropout) for etype in g.canonical_etypes}, \n",
        "                    aggregate=aggregator_hetero)\n",
        "                    )\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(n_layers - 2):\n",
        "            self.layers.append(\n",
        "                dglnn.HeteroGraphConv(\n",
        "                    {etype[1]: MessagePassing((dim_dict['hidden'], dim_dict['hidden']), dim_dict['hidden'], dropout) for etype in g.canonical_etypes},\n",
        "                    aggregate=aggregator_hetero)\n",
        "                    )\n",
        "\n",
        "        # output layer\n",
        "        self.layers.append(\n",
        "            dglnn.HeteroGraphConv(\n",
        "                {etype[1]: MessagePassing((dim_dict['hidden'], dim_dict['hidden']), dim_dict['out'], dropout) for etype in g.canonical_etypes}, \n",
        "                aggregate=aggregator_hetero)\n",
        "                )\n",
        "\n",
        "        if pred == 'cos':\n",
        "            self.pred_fn = CosinePrediction()\n",
        "        elif pred == 'nn':\n",
        "            self.pred_fn = NnPredictingModule(NnSimilarityPredictingLayer, dim_dict['out'])\n",
        "        elif pred == 'dotprod':\n",
        "            self.pred_fn = DotProductPredictor()\n",
        "        elif pred == 'pw':\n",
        "            self.pred_fn = PairWiseDistancePredictor()\n",
        "        else:\n",
        "            raise KeyError('Prediction function does not exist')\n",
        "            sys.exit(1)\n",
        "\n",
        "    def get_repr(self, blocks, h):\n",
        "\n",
        "        for i in range(len(blocks)):    \n",
        "            layer = self.layers[i]\n",
        "            h = layer(blocks[i], h)\n",
        "          \n",
        "        return h\n",
        "\n",
        "    def forward(self, blocks, h, pos_g, neg_g, embedding_layer: bool=True, ):\n",
        "        if embedding_layer:\n",
        "            h['user'] = self.user_embed(h['user'])\n",
        "            h['product'] = self.item_embed(h['product'])\n",
        "\n",
        "        h = self.get_repr(blocks, h)\n",
        "        pos_score = self.pred_fn(pos_g, h)\n",
        "        neg_score = self.pred_fn(neg_g, h)\n",
        "\n",
        "        return h, pos_score, neg_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxgfENcofcdh"
      },
      "source": [
        "## Định nghĩa hàm lưu các kết quả epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "dzQ2HpGUe6on"
      },
      "outputs": [],
      "source": [
        "def save_txt(data_to_save, filepath, mode='a'):\n",
        "\n",
        "    with open(filepath, mode) as text_file:\n",
        "        text_file.write(data_to_save + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Định nghĩa hàm loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrYf_KZVIy8p"
      },
      "source": [
        "### Max Margin Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "ZDB4RSsp_nRo"
      },
      "outputs": [],
      "source": [
        "def max_margin_loss(pos_score, neg_score, delta, neg_sample_size, negative_mask, cuda, device):\n",
        "\n",
        "    all_scores = torch.empty(0)\n",
        "\n",
        "    if cuda:\n",
        "        all_scores = all_scores.to(device)\n",
        "\n",
        "    for etype in pos_score.keys():\n",
        "        neg_score_tensor = neg_score[etype]\n",
        "        pos_score_tensor = pos_score[etype]\n",
        "        neg_score_tensor = neg_score_tensor.reshape(-1, neg_sample_size)\n",
        "        \n",
        "        negative_mask_tensor = negative_mask[etype].reshape(-1, neg_sample_size)\n",
        "       \n",
        "        if cuda:\n",
        "            negative_mask_tensor = negative_mask_tensor.to(device)\n",
        "        scores = (delta- pos_score_tensor + neg_score_tensor-negative_mask_tensor).clamp(min=0)\n",
        "        \n",
        "        if etype == edge_types[0]:\n",
        "            all_scores = torch.cat((all_scores, scores), 0)\n",
        "\n",
        "    return torch.mean(all_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAmTZKQqI29d"
      },
      "source": [
        "### Bayesian Personalized Ranking Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "XypRtiIHSoTz"
      },
      "outputs": [],
      "source": [
        "def bpr_loss(pos_score, neg_score, delta, neg_sample_size, negative_mask, cuda, device):\n",
        "\n",
        "    all_scores = torch.empty(0)\n",
        "\n",
        "    if cuda:\n",
        "        all_scores = all_scores.to(device)\n",
        "\n",
        "    for etype in pos_score.keys():\n",
        "        neg_score_tensor = neg_score[etype]\n",
        "        pos_score_tensor = pos_score[etype]\n",
        "        neg_score_tensor = neg_score_tensor.reshape(-1, neg_sample_size)\n",
        "\n",
        "        negative_mask_tensor = negative_mask[etype].reshape(-1, neg_sample_size)\n",
        "        \n",
        "\n",
        "        if cuda:\n",
        "            negative_mask_tensor = negative_mask_tensor.to(device)\n",
        "\n",
        "        logsig = nn.LogSigmoid()\n",
        "        scores = - logsig(pos_score_tensor - neg_score_tensor - negative_mask_tensor)\n",
        "        \n",
        "        if etype == edge_types[0]: \n",
        "          all_scores = torch.cat((all_scores, scores), 0)\n",
        "\n",
        "    return torch.mean(all_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc7CXglkI9Ev"
      },
      "source": [
        "### Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "CYV9IwcBp2CK"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(positive_score, negative_score, delta, negative_sample_size , negative_mask, cuda, device):\n",
        "    \n",
        "    all_scores = torch.empty(0)\n",
        "    \n",
        "    if cuda:\n",
        "        all_scores = all_scores.to(device)\n",
        "    \n",
        "    for edge_type in positive_score.keys():\n",
        "        negative_score_tensor = negative_score[edge_type].reshape(-1, 1)\n",
        "        positive_score_tensor = positive_score[edge_type]\n",
        "    \n",
        "        negative_mask_tensor = negative_mask[edge_type].reshape(-1, 1)\n",
        "    \n",
        "        if cuda:\n",
        "            negative_mask_tensor = negative_mask_tensor.to(device)\n",
        "    \n",
        "        relu = nn.ReLU()\n",
        "\n",
        "        if edge_type == edge_types[0]:\n",
        "            all_scores = torch.cat((all_scores, positive_score_tensor), 0)\n",
        "            negative_score_tensor = negative_score_tensor - negative_mask_tensor\n",
        "            all_scores = torch.cat((all_scores, negative_score_tensor), 0).reshape(-1, 1)\n",
        "            all_scores = relu(all_scores)\n",
        "            labels = torch.cat([torch.ones(positive_score_tensor.shape[0]), torch.zeros(negative_score_tensor.shape[0])]).reshape(-1, 1)\n",
        "\n",
        "            if cuda:\n",
        "                all_scores = all_scores.to(device) \n",
        "                labels = labels.to(device)\n",
        "        \n",
        "    loss = torch_nn_func.binary_cross_entropy_with_logits(all_scores, labels)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZV6xHXjQH1D"
      },
      "source": [
        "## Chia graph train thành các tập data để train và validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "szMr2FfT2qhF"
      },
      "outputs": [],
      "source": [
        "def train_validation_split(valid_graph, ground_truth_test, etypes, valid_size, reverse_etype):\n",
        "    # valid 0.15 - training 0.85\n",
        "    np.random.seed(11)\n",
        "\n",
        "    all_eids_dict, valid_eids_dict, train_eids_dict = {}, {}, {}\n",
        "    valid_uids_all, valid_iids_all = [], []\n",
        "\n",
        "    for etype in etypes:\n",
        "      all_eids = np.arange(valid_graph.number_of_edges(etype))\n",
        "      all_eids_dict[etype] = all_eids\n",
        "      # laasy index từ 0.85:1 -> tổng có 0.15\n",
        "      valid_eids = all_eids[int(len(all_eids) * (1 - valid_size)):]\n",
        "      valid_eids_dict[etype] = valid_eids\n",
        "    \n",
        "    all_eids_dict[etypes[0]] = all_eids\n",
        "    valid_eids = all_eids[int(len(all_eids) * (1 - valid_size)):]\n",
        "\n",
        "    valid_uids, valid_iids = valid_graph.find_edges(valid_eids, etype=etypes[0])\n",
        "    valid_uids_all.extend(valid_uids.tolist())\n",
        "    valid_iids_all.extend(valid_iids.tolist())\n",
        "        \n",
        "    ground_truth_valid = (np.array(valid_uids_all), np.array(valid_iids_all))\n",
        "    valid_uids = np.array(np.unique(valid_uids_all))\n",
        "\n",
        "    train_graph = valid_graph.clone()\n",
        "    \n",
        "    # từ graph ban đầu xóa đi các cạnh được validation -> còn cạnh để training \n",
        "    for etype in etypes:\n",
        "      train_graph.remove_edges(valid_eids_dict[etype], etype=etype)\n",
        "      train_eids = np.arange(train_graph.number_of_edges(etype))\n",
        "      train_eids_dict[etype] = train_eids\n",
        "\n",
        "    validation_graph = valid_graph.clone()\n",
        "\n",
        "    # từ graph ban đầu xóa đi các cạnh được training -> còn cạnh để validation \n",
        "    for etype in etypes:\n",
        "      validation_graph.remove_edges(train_eids_dict[etype], etype=etype)\n",
        "\n",
        "    train_uids, train_iids =train_graph.find_edges(train_eids_dict[etypes[0]], etype=etypes[0])\n",
        "    unique_train_uids = np.unique(train_uids)\n",
        "\n",
        "    ground_truth_train = (np.array(train_uids), np.array(train_iids))\n",
        "    train_uids = np.array(np.unique(train_uids))\n",
        "    test_uids, _ = ground_truth_test\n",
        "    test_uids = np.unique(test_uids)\n",
        "    all_iids = np.arange(valid_graph.num_nodes('product'))\n",
        "    \n",
        "    # Chuyển đổi tất cả các mảng numpy thành kiểu torch.int64\n",
        "    for etype in etypes:\n",
        "        all_eids_dict[etype] = torch.tensor(all_eids_dict[etype], dtype=torch.int64)\n",
        "        valid_eids_dict[etype] = torch.tensor(valid_eids_dict[etype], dtype=torch.int64)\n",
        "        train_eids_dict[etype] = torch.tensor(train_eids_dict[etype], dtype=torch.int64)\n",
        "\n",
        "    ground_truth_train = (torch.tensor(ground_truth_train[0], dtype=torch.int64), torch.tensor(ground_truth_train[1], dtype=torch.int64))\n",
        "    ground_truth_valid = (torch.tensor(ground_truth_valid[0], dtype=torch.int64), torch.tensor(ground_truth_valid[1], dtype=torch.int64))\n",
        "    train_uids = torch.tensor(train_uids, dtype=torch.int64)\n",
        "    valid_uids = torch.tensor(valid_uids, dtype=torch.int64)\n",
        "    test_uids = torch.tensor(test_uids, dtype=torch.int64)\n",
        "    all_iids = torch.tensor(all_iids, dtype=torch.int64)\n",
        "\n",
        "    return train_graph, train_eids_dict, valid_eids_dict, train_uids, valid_uids, test_uids, all_iids, ground_truth_train, ground_truth_valid, all_eids_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2RtL0Yy3oQX"
      },
      "source": [
        "## Chia tập data chia thành các batch nhỏ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class NegativeSampler(object):\n",
        "    def __init__(self, g, k):\n",
        "        self.g = g\n",
        "        self.k = k\n",
        "        self.low_rating_edges = (g.edges['rating'].data['rating'] < 3).nonzero().squeeze()\n",
        "\n",
        "        self.user_ids, self.product_ids = g.edges(etype='rating')\n",
        "        self.num_users = g.number_of_nodes('user')\n",
        "        self.num_products = g.number_of_nodes('product')\n",
        "\n",
        "    def __call__(self, g, eids_dict):\n",
        "        result_dict = {}\n",
        "        for etype, eids in eids_dict.items():\n",
        "            src, _ = g.find_edges(eids, etype=etype)\n",
        "            src = src.repeat_interleave(self.k)\n",
        "            if etype == 'rating':\n",
        "                # Create negative samples from low rating edges\n",
        "                filtered_dst_indices = self.low_rating_edges[\n",
        "                    torch.randint(0, len(self.low_rating_edges), (len(src),))\n",
        "                ]\n",
        "                dst_low_rating = self.user_ids[filtered_dst_indices]\n",
        "                \n",
        "                # Create negative samples from non-existent edges\n",
        "                neg_src = torch.randint(0, self.num_users, (len(src),))\n",
        "                neg_dst = torch.randint(0, self.num_products, (len(src),))\n",
        "                \n",
        "                # Ensure negative edges do not exist in the graph\n",
        "                mask = self.g.has_edges_between(neg_src, neg_dst, etype='rating')\n",
        "                while mask.any():\n",
        "                    neg_dst[mask] = torch.randint(0, self.num_products, (mask.sum().item(),))\n",
        "                    mask = self.g.has_edges_between(neg_src, neg_dst, etype='rating')\n",
        "                \n",
        "                dst = torch.cat([dst_low_rating, neg_dst], dim=0)\n",
        "                src = torch.cat([src, neg_src], dim=0)\n",
        "            else:\n",
        "                dst = torch.randint(0, self.num_products, (len(src),))\n",
        "            result_dict[etype] = (src, dst)\n",
        "        return result_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "r70EqCZtE8Eu"
      },
      "outputs": [],
      "source": [
        "def load_data(validation_graph, train_graph, train_edge_ids_dict, validation_edge_ids_dict, train_u_ids, validation_u_ids, test_u_ids, all_i_ids, \n",
        "                          number_workers, embedding_layer, number_layers, neighbor_sampler, neg_sample_size,  edge_batch_size, \n",
        "                          node_batch_size):\n",
        "    number_workers = 0 \n",
        "    if embedding_layer:\n",
        "        number_layers -= 1\n",
        "    if neighbor_sampler == 'full':\n",
        "        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(number_layers)\n",
        "    elif neighbor_sampler == 'partial':\n",
        "        sampler = dgl.dataloading.MultiLayerNeighborSampler([50, 40 ], replace=False)\n",
        "    else:\n",
        "        print('Neighbor sampler does not exit')\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Tạo bộ negative sampler tổng hợp\n",
        "    validation_sampler_n = NegativeSampler(validation_graph, neg_sample_size)\n",
        "    train_sampler_n = NegativeSampler(train_graph, neg_sample_size)\n",
        "    \n",
        "    # sampler_n = dgl.dataloading.negative_sampler.Uniform(neg_sample_size)\n",
        "    # train_sampler_n = sampler_n\n",
        "    # validation_sampler_n = sampler_n\n",
        "\n",
        "    #loading nodes\n",
        "    nodeLoad_train = dgl.dataloading.DataLoader(train_graph, {'user': train_u_ids, 'product': all_i_ids}, sampler, batch_size=node_batch_size, shuffle=True, \n",
        "                                                      drop_last=False, num_workers=number_workers,)\n",
        "\n",
        "    nodeLoad_validation = dgl.dataloading.DataLoader(validation_graph, {'user': validation_u_ids, 'product': all_i_ids}, sampler, batch_size=node_batch_size, \n",
        "                                                           shuffle=True, drop_last=False, num_workers=number_workers,)\n",
        "\n",
        "    \n",
        "    # Tạo edge prediction sampler cho training\n",
        "    train_sampler = as_edge_prediction_sampler(\n",
        "        sampler, exclude='reverse_types',\n",
        "        reverse_etypes={'rating': 'bought-by', 'bought-by': 'rating'},\n",
        "        negative_sampler=train_sampler_n\n",
        "    )\n",
        "\n",
        "    # Tạo DataLoader cho training\n",
        "    edgeLoad_train = dgl.dataloading.DataLoader(\n",
        "        train_graph, train_edge_ids_dict, train_sampler,\n",
        "        batch_size=edge_batch_size, shuffle=True, drop_last=False, num_workers=number_workers\n",
        "    )\n",
        "\n",
        "    # Tạo edge prediction sampler cho validation\n",
        "    validation_sampler = as_edge_prediction_sampler(\n",
        "        sampler, exclude='reverse_types',\n",
        "        reverse_etypes={'rating': 'bought-by', 'bought-by': 'rating'},\n",
        "        negative_sampler=validation_sampler_n\n",
        "    )\n",
        "\n",
        "    # Tạo DataLoader cho validation\n",
        "    edgeLoad_validation = dgl.dataloading.DataLoader(\n",
        "        validation_graph, validation_edge_ids_dict, validation_sampler,\n",
        "        batch_size=edge_batch_size, shuffle=True, drop_last=False, num_workers=number_workers\n",
        "    )\n",
        "\n",
        "    \n",
        "    test_node_ids = {'user': test_u_ids, 'product': all_i_ids}\n",
        "\n",
        "    nodeLoad_test = dgl.dataloading.DataLoader(validation_graph, test_node_ids, sampler, batch_size=node_batch_size, shuffle=True, drop_last=False, \n",
        "                                                     num_workers=number_workers)\n",
        "\n",
        "    return edgeLoad_train, edgeLoad_validation, nodeLoad_train, nodeLoad_validation, nodeLoad_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9pRpEAsQ8Fi"
      },
      "source": [
        "## Thông số cấu hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "vu-UGMfgOJpH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edge_types= [('user', 'rating', 'product'), ('product', 'bought-by', 'user')]\n",
        "reverse_edge_types= {('user', 'rating', 'product'): ('product', 'bought-by', 'user'), \n",
        "                 ('product', 'bought-by', 'user') : ('user', 'rating', 'product') }\n",
        "validation_size = .15\n",
        "cuda = torch.cuda.is_available()\n",
        "number_workers = 1 if cuda else 0\n",
        "device = torch.device('cuda') if cuda else torch.device('cpu')\n",
        "\n",
        "embedding_layer = True  \n",
        "number_layers = 3\n",
        "neighbor_sampler = 'partial' \n",
        "block_sampler = [50, 40 ]\n",
        "neg_sample_size = 15 \n",
        "edge_batch_size = 4096 \n",
        "node_batch_size = 128\n",
        "\n",
        "# GNN Conv Layer parameters:\n",
        "out_dim = 64 \n",
        "hidden_dim = 256\n",
        "validation_graph = hetero_graph\n",
        "prediction = 'cos' \n",
        "aggregator_hetero = 'mean' \n",
        "dropout = 0.3 \n",
        "\n",
        "# nn PredLayer parameters:\n",
        "hidden_layer_1_output_dim = 256 \n",
        "hidden_layer_2_output_dim = 128 \n",
        "\n",
        "num_epochs = 15 \n",
        "delta = 0.6 \n",
        "\n",
        "optimizer=torch.optim.Adam \n",
        "lr = 0.001 \n",
        "weight_decay = 1e-5\n",
        "loss_function = max_margin_loss \n",
        "patience = 20\n",
        "result_filepath = './result/output.txt'\n",
        "k = 10 #number of recommendations\n",
        "cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3, 4, 2,  ..., 1, 4, 4])"
            ]
          },
          "execution_count": 253,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_graph.edata['rating'][('product', 'bought-by', 'user')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3, 4, 2,  ..., 1, 4, 4])"
            ]
          },
          "execution_count": 254,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_graph.edges['bought-by'].data['rating']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzP-qUyrQ_t3"
      },
      "source": [
        "## Thực hiện chia dữ liệu để train và validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "VBYotwno0CYE"
      },
      "outputs": [],
      "source": [
        "train_graph, train_edge_ids_dict, validation_edge_ids_dict, train_u_ids, validation_u_ids, test_u_ids, all_i_ids, ground_truth_train, ground_truth_validation, all_edge_ids_dict = train_validation_split(hetero_graph, ground_truth_test, edge_types, validation_size, reverse_edge_types)\n",
        "\n",
        "edgeLoad_train, edgeLoad_validation, nodeLoad_train, nodeLoad_validation, nodeLoad_test = load_data(validation_graph, train_graph, train_edge_ids_dict, \n",
        "                                                                                                                      validation_edge_ids_dict, train_u_ids, validation_u_ids, \n",
        "                                                                                                                      test_u_ids, all_i_ids, number_workers,  \n",
        "                                                                                                                      embedding_layer, number_layers, neighbor_sampler, \n",
        "                                                                                                                      neg_sample_size = neg_sample_size, \n",
        "                                                                                                                      edge_batch_size = edge_batch_size,  # edge_batch_size = 4096,\n",
        "                                                                                                                     node_batch_size = node_batch_size ) # node_batch_size = 128 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ3hD-cfLJ3T"
      },
      "source": [
        "## Các Batches cho Train, Validation và Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "xxHjKucALmUA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 - 23 - 1 - 5 - 4\n"
          ]
        }
      ],
      "source": [
        "train_edge_ids_len = 0\n",
        "validation_edge_ids_len = 0\n",
        "\n",
        "for edge_type in train_edge_ids_dict.keys():\n",
        "    train_edge_ids_len += len(train_edge_ids_dict[edge_type])\n",
        "    validation_edge_ids_len += len(validation_edge_ids_dict[edge_type])\n",
        "\n",
        "num_batches_train_loss = math.ceil(train_edge_ids_len / edge_batch_size)\n",
        "num_batches_train_metrics = math.ceil((len(train_u_ids) + len(all_i_ids)) / node_batch_size)\n",
        "num_batches_validation_loss = math.ceil(validation_edge_ids_len /edge_batch_size)\n",
        "num_batches_validation_metrics = math.ceil((len(validation_u_ids) + len(all_i_ids)) / node_batch_size)\n",
        "num_batches_test = math.ceil((len(test_u_ids) + len(all_i_ids)) / node_batch_size)\n",
        "print(num_batches_train_loss, num_batches_train_metrics, num_batches_validation_loss, num_batches_validation_metrics, num_batches_test, sep=' - ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOY-D9J3kibM"
      },
      "source": [
        "## Hàm tính toán Embeddings dựa trên từng node load data đã chia từng batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "Vdyq5Y3jbZ4M"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(g, out_dim, trained_model, nodeLoad_test, num_batches_valid, cuda, device, embedding_layer):\n",
        "\n",
        "    if cuda:  \n",
        "        trained_model = trained_model.to(device)\n",
        "    i = 0\n",
        "    y = {ntype: torch.zeros(g.num_nodes(ntype), out_dim) for ntype in g.ntypes}\n",
        "    \n",
        "    if cuda: \n",
        "        y = {ntype: torch.zeros(g.num_nodes(ntype), out_dim).to(device) for ntype in g.ntypes}\n",
        "    \n",
        "    # print('yyyyyy', y)\n",
        "    for input_nodes, output_nodes, blocks in nodeLoad_test:\n",
        "        i += 1\n",
        "       \n",
        "        if i % 10 == 0:\n",
        "            # print(\"Computing embeddings: Batch \"+ str(i)+ \" out of \"  + str(num_batches_valid))\n",
        "            pass\n",
        "       \n",
        "        if cuda:\n",
        "            blocks = [b.to(device) for b in blocks]\n",
        "        \n",
        "        input_features = blocks[0].srcdata['features']\n",
        "        # print('input_features ', input_features)\n",
        "        if embedding_layer:\n",
        "            input_features['user'] = trained_model.user_embed(input_features['user'])\n",
        "            input_features['product'] = trained_model.item_embed(input_features['product'])\n",
        "        \n",
        "        h = trained_model.get_repr(blocks, input_features)\n",
        "        \n",
        "        for ntype in h.keys():\n",
        "            if ntype in output_nodes:\n",
        "                y[ntype][output_nodes[ntype]] = h[ntype]\n",
        "                # print('ntype ', ntype)\n",
        "                # print(output_nodes[ntype])\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqP_FYcRksdJ"
      },
      "source": [
        "## Hàm tính Metrics và Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "htK3x_2WcDH_"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(recs, ground_truth_dict, g):\n",
        "\n",
        "    k_relevant = 0\n",
        "    k_total = 0\n",
        "\n",
        "    for uid, iids in recs.items():\n",
        "        k_total += len(iids)\n",
        "        k_relevant += len([id_ for id_ in iids if id_ in ground_truth_dict[uid]])\n",
        "    \n",
        "    precision = k_relevant/k_total\n",
        "\n",
        "    nb_total = g.num_nodes('product')\n",
        "    recs_flatten = [item for sublist in list(recs.values()) for item in sublist]\n",
        "    nb_recommended = len(set(recs_flatten))\n",
        "    coverage = nb_recommended / nb_total\n",
        "    \n",
        "    return precision, coverage\n",
        "  \n",
        "def get_recom(g, h, model, embed_dim, k, user_ids, cuda, device, pred: str, epoch):\n",
        "\n",
        "    if cuda:  \n",
        "        model = model.to(device)\n",
        "\n",
        "    recom = {}\n",
        "\n",
        "    for user in user_ids:\n",
        "        user_emb = h['user'][user]\n",
        "        \n",
        "        user_emb_rpt = torch.cat(g.num_nodes('product') * [user_emb]).reshape(-1, embed_dim) \n",
        "        \n",
        "        if pred == 'cos':\n",
        "            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "            ratings = cos(user_emb_rpt, h['product'])\n",
        "        elif pred == 'nn':\n",
        "            cat_embed = torch.cat((user_emb_rpt, h['product']), 1)\n",
        "            ratings = model.pred_fn.layer_nn(cat_embed)\n",
        "        elif pred == 'dotprod':\n",
        "            ratings = torch.sum(user_emb_rpt * h['product'], dim=1)\n",
        "            print(\"ratings shape: \", ratings.shape)\n",
        "        elif pred == 'pw':\n",
        "            ratings =nn.PairwiseDistance(user_emb_rpt, h['product'])\n",
        "        else:\n",
        "            print ('the prediction function not found!')\n",
        "            sys.exit(1)\n",
        "            \n",
        "        ratings_formatted = ratings.cpu().detach().numpy().reshape(g.num_nodes('product'),)\n",
        "        order = np.argsort(-ratings_formatted)\n",
        "        \n",
        "        rec = order[:k] # top k recommendations\n",
        "        recom[user] = rec\n",
        "        \n",
        "    return recom\n",
        "\n",
        "def get_metrics(h, g, model, embed_dim, ground_truth, k, cuda=False, device=None, pred='cos', epoch=4):\n",
        "\n",
        "    users, items = ground_truth\n",
        "    user_ids = np.unique(users).tolist()\n",
        "    ground_truth_arr = np.stack((np.asarray(users), np.asarray(items)), axis=1)\n",
        "    ground_truth_dict = defaultdict(list)\n",
        "    for key, val in ground_truth_arr:\n",
        "        ground_truth_dict[key].append(val)\n",
        "    recs = get_recom(g, h, model, embed_dim, k, user_ids, cuda, device, pred, epoch)\n",
        "    \n",
        "    precision, coverage = calculate_metrics(recs, ground_truth_dict, g)\n",
        "    return precision, coverage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_JKy3sfLsJ_"
      },
      "source": [
        "## Hàm tính AUC\n",
        "AUC càng cao càng tốt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "cp41jia9Ln_1"
      },
      "outputs": [],
      "source": [
        "def compute_auc(scores):\n",
        "    edge_type = ('user', 'rating', 'product')\n",
        "    pos_score = scores[0][edge_type]\n",
        "    neg_score = scores[1][edge_type]\n",
        "\n",
        "    scores = torch.cat([pos_score, neg_score]).detach().cpu().numpy()\n",
        "    labels = torch.cat(\n",
        "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).detach().numpy()\n",
        "\n",
        "    return roc_auc_score(labels, scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR2p1Ys4Yvw6"
      },
      "source": [
        "## Hàm Train  Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "Dlixm7hFSsbk"
      },
      "outputs": [],
      "source": [
        "def train_model(model,\n",
        "                num_epochs,\n",
        "                num_batches_train_loss,\n",
        "                num_batches_validation_loss,\n",
        "                num_batches_val_metrics,\n",
        "                num_batches_train_metric,\n",
        "                edgeLoad_train,\n",
        "                edgeLoad_validation,\n",
        "                nodeLoad_validation,\n",
        "                nodeLoad_train,\n",
        "                k,\n",
        "                loss_function, \n",
        "                delta, \n",
        "                negative_sample_size, \n",
        "                cuda=False,\n",
        "                device=None,\n",
        "                optimizer=torch.optim.Adam,\n",
        "                lr=0.001,\n",
        "                train_graph=None,\n",
        "                validation_graph=None,\n",
        "                out_dim=None, \n",
        "                ground_truth_train=None,\n",
        "                ground_truth_validation=None,\n",
        "                result_filepath=None,\n",
        "                patience=5,\n",
        "                prediction=None,\n",
        "                embedding_layer=True,\n",
        "                ):\n",
        "\n",
        "    model.train_loss_list, model.train_precision_list, model.train_coverage_list, model.validation_loss_list, model.validation_precision_list, model.validation_coverage_list = [], [], [], [], [], []\n",
        "    best_metrics = {} \n",
        "    \n",
        "    max_metric = - 0.1\n",
        "    patience_counter = 0  \n",
        "    min_loss = 1.1\n",
        "    max_auc = 0.5\n",
        "    best_epoch = 0\n",
        "    opt = optimizer(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    start = time.time()\n",
        "    print('Start training for '+ str(num_epochs)+ ' epochs')\n",
        "    \n",
        "    for epoch in range(1, num_epochs):\n",
        "        print('Epoch ', epoch)\n",
        "        if epoch == 1:\n",
        "            mode = 'w'  \n",
        "        else:\n",
        "            mode = 'a'  \n",
        "\n",
        "        model.train()  \n",
        "        i = 0\n",
        "        total_loss = 0\n",
        "        for _, pos_g, neg_g, blocks in edgeLoad_train:\n",
        "            \n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Negative mask\n",
        "            negative_mask = {}\n",
        "\n",
        "            nids = neg_g.ndata[dgl.NID]\n",
        "\n",
        "            for etype in pos_g.canonical_etypes:\n",
        "                neg_src, neg_dst = neg_g.edges(etype=etype)\n",
        "                neg_src = nids[etype[0]][neg_src]\n",
        "                neg_dst = nids[etype[2]][neg_dst]\n",
        "                negative_mask_tensor = train_graph.has_edges_between(neg_src, neg_dst, etype=etype)\n",
        "                negative_mask[etype] = negative_mask_tensor.type(torch.float)\n",
        "\n",
        "                if cuda:\n",
        "                    negative_mask[etype] = negative_mask[etype].to(device)\n",
        "\n",
        "            if cuda:\n",
        "                blocks = [b.to(device) for b in blocks]\n",
        "                pos_g = pos_g.to(device)\n",
        "                neg_g = neg_g.to(device)\n",
        "\n",
        "            i += 1\n",
        "            \n",
        "            if i % 10 == 0:\n",
        "                # print(\"Edge batch \" + str(i) + \" out of \", str(num_batches_train_loss))\n",
        "                pass\n",
        "\n",
        "            input_features = blocks[0].srcdata['features']\n",
        "\n",
        "            _, pos_score, neg_score = model(blocks, input_features, pos_g, neg_g, embedding_layer,)\n",
        "            train_score = (pos_score, neg_score)\n",
        "            loss = loss_function(pos_score, neg_score, delta, neg_sample_size, negative_mask=negative_mask, cuda=cuda, device=device,)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_avg_loss = total_loss / i\n",
        "        model.train_loss_list.append(train_avg_loss)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_loss = 0\n",
        "            i = 0\n",
        "\n",
        "            for _, pos_g, neg_g, blocks in edgeLoad_validation:\n",
        "                i += 1\n",
        "\n",
        "                if i % 10 == 0:\n",
        "                    # print(\"Edge batch {} out of {}\".format(i, num_batches_validation_loss))\n",
        "                    pass\n",
        "\n",
        "                # Negative mask\n",
        "                negative_mask = {}\n",
        "\n",
        "                nids = neg_g.ndata[dgl.NID]\n",
        "\n",
        "                for etype in pos_g.canonical_etypes:\n",
        "                    neg_src, neg_dst = neg_g.edges(etype=etype)\n",
        "                    neg_src = nids[etype[0]][neg_src]\n",
        "                    neg_dst = nids[etype[2]][neg_dst]\n",
        "                    negative_mask_tensor = validation_graph.has_edges_between(neg_src, neg_dst, etype=etype)\n",
        "                    negative_mask[etype] = negative_mask_tensor.type(torch.float)\n",
        "\n",
        "                    if cuda:\n",
        "                        negative_mask[etype] = negative_mask[etype].to(device)\n",
        "\n",
        "                if cuda:\n",
        "                    blocks = [b.to(device) for b in blocks]\n",
        "                    pos_g = pos_g.to(device)\n",
        "                    neg_g = neg_g.to(device)\n",
        "\n",
        "                input_features = blocks[0].srcdata['features']\n",
        "                _, pos_score, neg_score = model(blocks, input_features, pos_g, neg_g, embedding_layer,)\n",
        "                \n",
        "                validation_score = (pos_score, neg_score)\n",
        "\n",
        "                validation_loss = loss_function(pos_score, neg_score, delta, neg_sample_size, negative_mask=negative_mask, \n",
        "                                   cuda=cuda, device=device,)\n",
        "                total_loss += validation_loss.item()\n",
        "\n",
        "            validation_avg_loss = total_loss / i\n",
        "            model.validation_loss_list.append(validation_avg_loss)\n",
        "\n",
        "        model.eval()\n",
        "        AUC_val = 0\n",
        "        with torch.no_grad():\n",
        "            y = get_embeddings(train_graph, out_dim, model, nodeLoad_train, num_batches_train_metrics, cuda, device, embedding_layer,)\n",
        "            train_precision, train_coverage = get_metrics(y, train_graph, model, out_dim, ground_truth_train, k, \n",
        "                                                                cuda, device, prediction, epoch)\n",
        "\n",
        "            # validation metrics\n",
        "            y = get_embeddings(validation_graph,\n",
        "                                out_dim,\n",
        "                                model,\n",
        "                                nodeLoad_validation,\n",
        "                                num_batches_val_metrics,\n",
        "                                cuda,\n",
        "                                device,\n",
        "                                embedding_layer,\n",
        "                                )\n",
        "\n",
        "            validation_precision, validation_coverage = get_metrics(y, validation_graph, model, out_dim, ground_truth_validation,  k, \n",
        "                                                                          cuda, device, prediction, epoch)\n",
        "            AUC_val = compute_auc(validation_score)\n",
        "            AUC_train = compute_auc(train_score)\n",
        "            results = \"Epoch \" + str(epoch) +\" | Training loss \" + str(round(train_avg_loss,4)) +\" | Training Precision \" + str(round(train_precision * 100,2)) + \" | Training AUC \" + str(round(AUC_train,2))+ ' | Validation loss '+ str(round(validation_avg_loss,4)) + ' | Validation Precision '+str(round(validation_precision * 100,2)) + ' | AUC '+str(round(AUC_val,2))\n",
        "            print(results)\n",
        "            \n",
        "            print('process time: ' +str(round(float(time.time()-start)/60, 2))+ ' minutes')\n",
        "            \n",
        "            save_txt(results, result_filepath, mode=mode)\n",
        "\n",
        "            model.train_precision_list.append(train_precision * 100)\n",
        "            model.validation_precision_list.append(validation_precision * 100)\n",
        "\n",
        "            if validation_precision > max_metric:\n",
        "                max_metric = validation_precision\n",
        "                best_metrics = { 'precision': validation_precision}\n",
        "\n",
        "        # if max_auc < AUC_val:\n",
        "        #     max_auc = AUC_val\n",
        "        #     best_model = copy.deepcopy(model)\n",
        "        #     best_epoch = epoch\n",
        "\n",
        "        if validation_avg_loss < min_loss:\n",
        "            min_loss = validation_avg_loss\n",
        "            patience_counter = 0\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_epoch = epoch\n",
        "            # print('patience_counter = ', patience_counter)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            # print('patience_counter = ', patience_counter)\n",
        "\n",
        "        # if patience_counter == patience:\n",
        "        #     print('ngắt training')\n",
        "        #     break\n",
        "\n",
        "    viz_dict = {'train_loss_list': model.train_loss_list,\n",
        "           'train_precision_list': model.train_precision_list,\n",
        "           'val_loss_list': model.validation_loss_list,\n",
        "           'validation_precision_list': model.validation_precision_list,\n",
        "          }\n",
        "    \n",
        "    print('end of training!!')\n",
        "    print ('process time: ' +str(round(float(time.time()-start)/60, 2))+ ' minutes')\n",
        "    print('best epoch: ', best_epoch)\n",
        "    return best_model, viz_dict, validation_score, train_score, best_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Graph(num_nodes={'product': 33, 'user': 3334},\n",
              "      num_edges={('product', 'bought-by', 'user'): 11468, ('user', 'rating', 'product'): 11468},\n",
              "      metagraph=[('product', 'user', 'bought-by'), ('user', 'product', 'rating')])"
            ]
          },
          "execution_count": 261,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "edgeLoad_train.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey2FHOQjm9-F",
        "outputId": "9c2b0c08-c055-4cc7-d115-6c2b0f6024be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for 1 epochs\n",
            "end of training!!\n",
            "process time: 0.0 minutes\n",
            "best epoch:  0\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'best_model' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[263], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cuda:\n\u001b[0;32m      9\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m trained_model, viz_dict, validation_score, train_score, best_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                                       \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                                      \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mnum_batches_train_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mnum_batches_validation_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mnum_batches_validation_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mnum_batches_train_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43medgeLoad_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43medgeLoad_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mnodeLoad_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mnodeLoad_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# số lượng recommend\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mneg_sample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mtrain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mvalidation_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mout_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mground_truth_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mground_truth_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mresult_filepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                                                                        \u001b[49m\u001b[43membedding_layer\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                                                                      \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[260], line 206\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, num_epochs, num_batches_train_loss, num_batches_validation_loss, num_batches_val_metrics, num_batches_train_metric, edgeLoad_train, edgeLoad_validation, nodeLoad_validation, nodeLoad_train, k, loss_function, delta, negative_sample_size, cuda, device, optimizer, lr, train_graph, validation_graph, out_dim, ground_truth_train, ground_truth_validation, result_filepath, patience, prediction, embedding_layer)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocess time: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest epoch: \u001b[39m\u001b[38;5;124m'\u001b[39m, best_epoch)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbest_model\u001b[49m, viz_dict, validation_score, train_score, best_metrics\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'best_model' referenced before assignment"
          ]
        }
      ],
      "source": [
        "dim_dict = {'user': validation_graph.nodes['user'].data['features'].shape[1],\n",
        "            'product': validation_graph.nodes['product'].data['features'].shape[1],\n",
        "            'out':  out_dim,\n",
        "            'hidden':hidden_dim}\n",
        "\n",
        "model = GNNModel(validation_graph, number_layers, dim_dict, dropout, prediction, aggregator_hetero, embedding_layer)\n",
        "\n",
        "if cuda:\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "trained_model, viz_dict, validation_score, train_score, best_metrics = train_model(model, \n",
        "                                                                       2,                                                      \n",
        "                                                                        num_batches_train_loss, \n",
        "                                                                        num_batches_validation_loss,\n",
        "                                                                        num_batches_validation_metrics,\n",
        "                                                                        num_batches_train_metrics, \n",
        "                                                                        edgeLoad_train, \n",
        "                                                                        edgeLoad_validation,\n",
        "                                                                        nodeLoad_validation, \n",
        "                                                                        nodeLoad_train, \n",
        "                                                                        k, # số lượng recommend\n",
        "                                                                        loss_function, \n",
        "                                                                        delta, \n",
        "                                                                        neg_sample_size,\n",
        "                                                                        cuda,\n",
        "                                                                        device,\n",
        "                                                                        optimizer,\n",
        "                                                                        lr,\n",
        "                                                                        train_graph, \n",
        "                                                                        validation_graph,\n",
        "                                                                        out_dim, \n",
        "                                                                        ground_truth_train, \n",
        "                                                                        ground_truth_validation, \n",
        "                                                                        result_filepath,\n",
        "                                                                        patience,\n",
        "                                                                        prediction,\n",
        "                                                                        embedding_layer\n",
        "                                                                      )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL2fIFYoX1D3"
      },
      "source": [
        "## Testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_inference(valid_graph, out_dim: int, trained_model, nodeLoad_test, num_batches_test: int, cuda: bool, device, embedding_layer: bool, \n",
        "                    ground_truth_test, all_eids_dict):\n",
        "\n",
        "  trained_model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "        embeddings = get_embeddings(valid_graph, out_dim, trained_model, nodeLoad_test, num_batches_test, cuda, device, embedding_layer,)\n",
        "        print(embeddings['product'].size())\n",
        "        print(embeddings['user'].size())\n",
        "        precision,_ = get_metrics(embeddings, valid_graph, trained_model, out_dim, ground_truth_test, k, cuda, device, prediction, epoch = 4)\n",
        "        sentence = \" TEST Precision {:.3f}% \".format(precision * 100)\n",
        "        print(sentence)\n",
        "        save_txt(sentence, result_filepath, mode= 'a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_inference(validation_graph, out_dim, trained_model, nodeLoad_test, num_batches_test, cuda, device, embedding_layer, ground_truth_test, all_edge_ids_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save graph, model và draw kết quả"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dgl.save_graphs('graph.dgl', validation_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(trained_model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loV9z38qT8Jx"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "x = list(range(1,len(viz_dict['train_loss_list'])+1))\n",
        "plt.title('\\n'.join(textwrap.wrap('train and validation loss', 60)))\n",
        "fig.tight_layout()\n",
        "plt.rcParams[\"axes.titlesize\"] = 6\n",
        "plt.plot(x, viz_dict['train_loss_list'])\n",
        "plt.plot(x, viz_dict['val_loss_list'])\n",
        "plt.legend(['training loss', 'valid loss'], loc='upper left')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.savefig('./result/' + 'loss.png')\n",
        "plt.close(fig)\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "x = list(range(1,len(viz_dict['train_precision_list'])+1))\n",
        "\n",
        "plt.title('\\n'.join(textwrap.wrap('train and validation precision', 60)))\n",
        "fig.tight_layout()\n",
        "plt.rcParams[\"axes.titlesize\"] = 6\n",
        "plt.plot(x, viz_dict['train_precision_list'])\n",
        "plt.plot(x, viz_dict['validation_precision_list'])\n",
        "plt.legend(['training precision','valid precision'], loc='upper left')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Percentage of metrics(%)')\n",
        "plt.savefig('./result/' + 'metrics.png')\n",
        "plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model và graph nếu đã lưu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction = 'cos' \n",
        "aggregator_hetero = 'mean' \n",
        "dropout = 0.3 \n",
        "embedding_layer = True\n",
        "\n",
        "validation_graph, _ = dgl.load_graphs('graph_train.dgl')\n",
        "validation_graph = validation_graph[0]\n",
        "\n",
        "dim_dict = {'user': validation_graph.nodes['user'].data['features'].shape[1],\n",
        "            'product': validation_graph.nodes['product'].data['features'].shape[1],\n",
        "            'out':  out_dim,\n",
        "            'hidden':hidden_dim}\n",
        "\n",
        "trained_model = GNNModel(validation_graph, 3, dim_dict, dropout, prediction, aggregator_hetero, embedding_layer)\n",
        "\n",
        "# Load trọng số đã lưu vào mô hình khởi tạo\n",
        "trained_model.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "# Chuyển mô hình sang thiết bị cần thiết (nếu cần)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "trained_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test recommend với full embedded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_full_graph_embeddings(g, trained_model, device, embedding_layer):\n",
        "    # Ensure the graph is on the correct device\n",
        "    g = g.to(device)\n",
        "    \n",
        "    h = g.ndata['features']\n",
        "        \n",
        "    if embedding_layer:\n",
        "        \n",
        "        # Assuming 'user' and 'product' are keys in input_features\n",
        "        h['user'] = trained_model.user_embed(h['user'].to(device))\n",
        "        h['product'] = trained_model.item_embed(h['product'].to(device))\n",
        "    \n",
        "    # Calculate representations\n",
        "    with torch.no_grad():  \n",
        "        for i in range(len(trained_model.layers)):\n",
        "            # print(trained_model.layers[i])\n",
        "            h = trained_model.layers[i](g, h)\n",
        "    with open('./embeddings.pkl', 'wb') as f:\n",
        "        pickle.dump(h, f)\n",
        "    return h\n",
        "\n",
        "# Assuming 'validation_graph' is already on the correct device\n",
        "embeddings_full = get_full_graph_embeddings(validation_graph, trained_model, device, embedding_layer)\n",
        "\n",
        "# Example call to `get_recom` with embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id_table[user_id_table['UserID'] == 'U00006035']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interact_df[interact_df['Rating'] < 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_recom(validation_graph, embeddings_full, trained_model, out_dim, 33, [3], cuda, device, 'cos', 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Thêm user node và edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_new_user_feat(graph, user_feature):\n",
        "    temple = graph.ndata['features']['user'].clone()\n",
        "    graph = dgl.add_nodes(graph, 1, ntype='user')\n",
        "    graph.nodes['user'].data['features'] = torch.cat((temple, user_feature.unsqueeze(0)), dim=0)\n",
        "    return graph\n",
        "\n",
        "def add_new_edge(graph, src_id, dest_id,edge_feature, etype):\n",
        "    graph.add_edges(src_id, dest_id,  data = edge_feature, etype = etype)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_user_df = pd.read_csv('./collect_data/user.csv')\n",
        "trained_user_df = pd.read_csv('./user_id_table.csv')\n",
        "print(all_user_df.shape)\n",
        "print(trained_user_df.shape)\n",
        "print(all_user_df.shape[0] - trained_user_df.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tìm các UserID chỉ có trong all_user_df mà không có trong trained_user_df\n",
        "merged_df = pd.merge(all_user_df, trained_user_df, on='UserID', how='left', indicator=True)\n",
        "not_trained_user_df = merged_df.loc[merged_df['_merge'] == 'left_only', ['UserID', 'F', 'M']]\n",
        "not_trained_user_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(rating_all_df.shape)\n",
        "# do rating train bị drop trùng lặp nên giá trị ban đầu có thể lớn hơn 11468 => edge chưa train có nhỏ hơn 4461\n",
        "print(rating_train_df.shape)\n",
        "print(rating_all_df.shape[0] - rating_train_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_trained_user_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_edge_trained_df = pd.merge(not_trained_user_df, rating_all_df, on='UserID', how='inner', indicator=True)\n",
        "not_edge_trained_df = pd.merge(not_edge_trained_df, product_id_all_df, on='ItemID', how='inner', indicator=False)\n",
        "not_edge_trained_df.head()\n",
        "not_edge_trained_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data05 = not_edge_trained_df.loc[not_edge_trained_df['UserID'] == 'U00000005']\n",
        "data05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_graph = validation_graph.clone()\n",
        "x_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_user_id = trained_user_df['user_new_id'].max() if not trained_user_df.empty else 0\n",
        "\n",
        "user05_data = not_trained_user_df.loc[not_trained_user_df['UserID'] == 'U00000005']\n",
        "user05_data = user05_data.reset_index(drop=True)\n",
        "user05_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_user_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Thêm thử 1 node user U00000005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user05_data['F'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "user_id = user05_data['UserID'][0]\n",
        "\n",
        "features = torch.tensor([user05_data['F'][0], user05_data['M'][0]])  # Tạo tensor đặc trưng từ 'F' và 'M'\n",
        "\n",
        "validation_graph = add_new_user_feat(validation_graph, features)\n",
        "\n",
        "max_user_id += 1\n",
        "trained_user_df.loc[len(trained_user_df)] = [user_id, max_user_id]\n",
        "validation_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Thêm thử cạnh của user U00000005"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id_05_mapped = trained_user_df['user_new_id'].max() if not trained_user_df.empty else 0\n",
        "user_id_05_mapped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index, row in data05.iterrows():\n",
        "    data05\n",
        "    src = user_id_05_mapped\n",
        "    dst = row['product_new_id']\n",
        "    feat = torch.tensor([row['Rating']])\n",
        "    validation_graph = add_new_edge(validation_graph, src, dst, {'rating': feat}, 'rating')\n",
        "    \n",
        "for index, row in data05.iterrows():\n",
        "    data05\n",
        "    dst = user_id_05_mapped\n",
        "    src = row['product_new_id']\n",
        "    feat = torch.tensor([row['Rating']])\n",
        "    validation_graph = add_new_edge(validation_graph, src, dst, {'rating': feat}, 'bought-by')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_user_id = trained_user_df['user_new_id'].max() if not trained_user_df.empty else 0\n",
        "# for index, row in not_trained_user_df.iterrows():\n",
        "#     user_id = row['UserID']\n",
        "#     features = torch.tensor([row['F'], row['M']])  # Tạo tensor đặc trưng từ 'F' và 'M'\n",
        "\n",
        "#     validation_graph = add_new_user_feat(validation_graph, features)\n",
        "\n",
        "#     max_user_id += 1\n",
        "#     trained_user_df.loc[len(trained_user_df)] = [user_id, max_user_id]\n",
        "\n",
        "validation_graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Thêm node và edges toàn bộ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_new_user_feat(graph, user_feature):\n",
        "    temple = graph.ndata['features']['user'].clone()\n",
        "    graph = dgl.add_nodes(graph, 1, ntype='user')\n",
        "    graph.nodes['user'].data['features'] = torch.cat((temple, user_feature.unsqueeze(0)), dim=0)\n",
        "    return graph\n",
        "\n",
        "def add_new_edge(graph, src_id, dest_id,edge_feature, etype):\n",
        "    graph.add_edges(src_id, dest_id,  data = edge_feature, etype = etype)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_user_df = pd.read_csv('./collect_data/user.csv')\n",
        "trained_user_df = pd.read_csv('./user_id_table.csv')\n",
        "print(all_user_df.shape)\n",
        "print(trained_user_df.shape)\n",
        "print(all_user_df.shape[0] - trained_user_df.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tìm các UserID chỉ có trong all_user_df mà không có trong trained_user_df\n",
        "merged_df = pd.merge(all_user_df, trained_user_df, on='UserID', how='left', indicator=True)\n",
        "not_trained_user_df = merged_df.loc[merged_df['_merge'] == 'left_only', ['UserID', 'F', 'M']]\n",
        "not_trained_user_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_edge_trained_df = pd.merge(not_trained_user_df, rating_all_df, on='UserID', how='inner', indicator=True)\n",
        "not_edge_trained_df = pd.merge(not_edge_trained_df, product_id_all_df, on='ItemID', how='inner', indicator=False)\n",
        "not_edge_trained_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_edge_trained_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tìm các UserID chỉ có trong all_user_df mà không có trong trained_user_df\n",
        "merged_df = pd.merge(all_user_df, trained_user_df, on='UserID', how='left', indicator=True)\n",
        "not_trained_user_df = merged_df.loc[merged_df['_merge'] == 'left_only', ['UserID', 'F', 'M']]\n",
        "not_trained_user_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_user_id = trained_user_df['user_new_id'].max() if not trained_user_df.empty else 0\n",
        "for index, row in not_trained_user_df.iterrows():\n",
        "    user_id = row['UserID']\n",
        "    features = torch.tensor([row['F'], row['M']])  # Tạo tensor đặc trưng từ 'F' và 'M'\n",
        "\n",
        "    validation_graph = add_new_user_feat(validation_graph, features)\n",
        "\n",
        "    max_user_id += 1\n",
        "    trained_user_df.loc[len(trained_user_df)] = [user_id, max_user_id]\n",
        "\n",
        "validation_graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index, row in not_edge_trained_df.iterrows():\n",
        "    user_new_id_value = trained_user_df[trained_user_df['UserID'] == row['UserID']]['user_new_id'].iloc[0]\n",
        "    \n",
        "    src = user_new_id_value\n",
        "    dst = row['product_new_id']\n",
        "    feat = torch.tensor([row['Rating']])\n",
        "    validation_graph = add_new_edge(validation_graph, src, dst, {'rating': feat}, 'rating')\n",
        "    \n",
        "    src2 = row['product_new_id']\n",
        "    dst2 = user_new_id_value\n",
        "    feat2 = torch.tensor([row['Rating']])\n",
        "    validation_graph = add_new_edge(validation_graph, src2, dst2, {'rating': feat2}, 'bought-by')\n",
        "    \n",
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_user_df.to_csv('user_id_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tính embedded dựa trên nodeLoad_test và recommend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = get_embeddings(validation_graph, out_dim, trained_model, nodeLoad_test, num_batches_test, cuda, device, embedding_layer,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "get_recom(validation_graph, embeddings, trained_model, out_dim, k, [0, 3648, 26, 44, 53, 73, 7, 23], cuda, device, 'cos', 4)\n",
        "\n",
        "# precision,_ = get_metrics(embeddings, validation_graph, trained_model, out_dim, ground_truth_test, k, cuda, device, prediction, epoch = 4)\n",
        "# sentence = \" TEST Precision {:.3f}% \".format(precision * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vẽ biểu đồ các node user, product embedded 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dgl\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "\n",
        "# Giả sử bạn có một mô hình đã được huấn luyện và validation_graph\n",
        "\n",
        "\n",
        "# Sử dụng hàm để lấy embeddings cho toàn bộ đồ thị\n",
        "full_embeddings = get_full_graph_embeddings(validation_graph, trained_model, device, True)\n",
        "\n",
        "# Xử lý nhúng cho từng loại node\n",
        "user_embeddings = full_embeddings['user'].cpu().numpy()\n",
        "product_embeddings = full_embeddings['product'].cpu().numpy()\n",
        "\n",
        "# Sử dụng TSNE để giảm chiều dữ liệu cho từng loại node\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "user_embeddings_2d = tsne.fit_transform(user_embeddings)\n",
        "product_embeddings_2d = tsne.fit_transform(product_embeddings)\n",
        "\n",
        "# Sử dụng KMeans để phân cụm các node user\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)  # Số cụm tùy chỉnh\n",
        "user_labels = kmeans.fit_predict(user_embeddings_2d)\n",
        "product_labels = kmeans.fit_predict(product_embeddings_2d)\n",
        "\n",
        "# Tạo DataFrame cho việc vẽ\n",
        "import pandas as pd\n",
        "user_df = pd.DataFrame(user_embeddings_2d, columns=['x', 'y'])\n",
        "user_df['label'] = user_labels\n",
        "user_df['type'] = 'user'\n",
        "\n",
        "product_df = pd.DataFrame(product_embeddings_2d, columns=['x', 'y'])\n",
        "product_df['label'] = product_labels\n",
        "product_df['type'] = 'product'\n",
        "\n",
        "# Kết hợp dữ liệu user và product\n",
        "df = pd.concat([user_df, product_df])\n",
        "\n",
        "# Vẽ biểu đồ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(data=df, x='x', y='y', hue='label', style='type', palette='tab10', s=100, legend='full')\n",
        "\n",
        "plt.title('Node Embeddings Visualization with Clustering')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bokeh.models import ColumnDataSource, HoverTool\n",
        "from bokeh.plotting import figure, show, output_notebook\n",
        "from bokeh.transform import factor_cmap\n",
        "from bokeh.palettes import Category10\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "\n",
        "# Giả sử bạn có một mô hình đã được huấn luyện và validation_graph\n",
        "full_embeddings = get_full_graph_embeddings(validation_graph, trained_model, device, True)\n",
        "\n",
        "# Tạo dữ liệu giả để minh họa\n",
        "user_embeddings = full_embeddings['user'].cpu().numpy()\n",
        "product_embeddings = full_embeddings['product'].cpu().numpy()\n",
        "\n",
        "\n",
        "# Sử dụng TSNE để giảm chiều dữ liệu cho từng loại node\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "user_embeddings_2d = tsne.fit_transform(user_embeddings)\n",
        "product_embeddings_2d = tsne.fit_transform(product_embeddings)\n",
        "\n",
        "# Sử dụng KMeans để phân cụm các node user và product\n",
        "kmeans_users = KMeans(n_clusters=5, random_state=42)  # Số cụm tùy chỉnh\n",
        "kmeans_products = KMeans(n_clusters=5, random_state=42)\n",
        "\n",
        "user_labels = kmeans_users.fit_predict(user_embeddings_2d)\n",
        "product_labels = kmeans_products.fit_predict(product_embeddings_2d)\n",
        "\n",
        "# Tạo DataFrame cho việc vẽ\n",
        "user_df = pd.DataFrame(user_embeddings_2d, columns=['x', 'y'])\n",
        "user_df['label'] = user_labels\n",
        "user_df['type'] = 'user'\n",
        "\n",
        "product_df = pd.DataFrame(product_embeddings_2d, columns=['x', 'y'])\n",
        "product_df['label'] = product_labels\n",
        "product_df['type'] = 'product'\n",
        "\n",
        "# Tạo nguồn dữ liệu cho Bokeh\n",
        "source_user = ColumnDataSource(data=dict(\n",
        "    x=user_df['x'],\n",
        "    y=user_df['y'],\n",
        "    label=user_df['label'].astype(str),  # Convert labels to string for categorical coloring\n",
        "    type=user_df['type']\n",
        "))\n",
        "\n",
        "source_product = ColumnDataSource(data=dict(\n",
        "    x=product_df['x'],\n",
        "    y=product_df['y'],\n",
        "    label=product_df['label'].astype(str),\n",
        "    type=product_df['type']\n",
        "))\n",
        "\n",
        "# Tạo màu sắc cho các nhãn phân cụm\n",
        "unique_user_labels = user_df['label'].unique().astype(str)\n",
        "unique_product_labels = product_df['label'].unique().astype(str)\n",
        "\n",
        "user_color_mapping = factor_cmap('label', palette=Category10[len(unique_user_labels)], factors=unique_user_labels)\n",
        "product_color_mapping = factor_cmap('label', palette=Category10[len(unique_product_labels)], factors=unique_product_labels)\n",
        "\n",
        "# Tạo figure cho Bokeh\n",
        "p = figure(title=\"Node Embeddings Visualization with Clustering\", tools=\"pan,wheel_zoom,box_zoom,reset,hover,save\", tooltips=\"@type: @label\")\n",
        "\n",
        "# Thêm scatter plot với màu sắc dựa trên nhãn phân cụm\n",
        "p.scatter('x', 'y', source=source_user, legend_field='type', fill_alpha=0.6, size=8, color=user_color_mapping, marker='circle')\n",
        "p.scatter('x', 'y', source=source_product, legend_field='type', fill_alpha=0.6, size=8, color=product_color_mapping, marker='triangle')\n",
        "\n",
        "# Cấu hình tooltips\n",
        "hover = p.select(dict(type=HoverTool))\n",
        "hover.tooltips = [\n",
        "    (\"Index\", \"$index\"),\n",
        "    (\"(x, y)\", \"(@x, @y)\"),\n",
        "    (\"Type\", \"@type\"),\n",
        "    (\"Label\", \"@label\"),\n",
        "]\n",
        "\n",
        "# Hiển thị biểu đồ\n",
        "output_notebook()\n",
        "show(p, notebook_handle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## define recommend function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_for_user(user_id, validation_graph, embeddings, out_dim, k):\n",
        "    user_emb = embeddings['user'][user_id]\n",
        "    print(user_emb.size())\n",
        "    user_emb_rpt = user_emb.repeat(validation_graph.num_nodes('product'), 1)\n",
        "    \n",
        "    print(user_emb_rpt.size())\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    ratings = cos(user_emb_rpt, embeddings['product'])\n",
        "    \n",
        "    ratings_formatted = ratings.cpu().detach().numpy().reshape(validation_graph.num_nodes('product'),)\n",
        "    order = np.argsort(-ratings_formatted)\n",
        "    \n",
        "    rec = order[:k]  # top k recommendations\n",
        "    return rec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_embedded_and_recommend(user_id, validation_graph, trained_model, out_dim, device, k):\n",
        "    embeddings = get_embeddings(validation_graph, out_dim, trained_model, nodeLoad_test, num_batches_test, cuda, device, embedding_layer)\n",
        "    \n",
        "    recommended_products = recommend_for_user(user_id, validation_graph, embeddings, out_dim, k)\n",
        "    print(user_id, ' - ' , recommended_products)\n",
        "    return recommended_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = get_embeddings(validation_graph, out_dim, trained_model, nodeLoad_train, num_batches_test, cuda, device, embedding_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_recom(validation_graph, embeddings, trained_model, out_dim, k, [0, 12, 2, 3, 4], cuda, device, 'cos', 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Thêm các user còn lại vào graph all (Vì các user này chưa rating nên lúc đầu graph all không có chứa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_user_df = pd.read_csv('./collect_data/user.csv')\n",
        "trained_user_df = pd.read_csv('./user_id_table.csv')\n",
        "print(all_user_df.shape)\n",
        "print(trained_user_df.shape)\n",
        "print(all_user_df.shape[0] - trained_user_df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tìm các UserID chỉ có trong all_user_df mà không có trong trained_user_df\n",
        "merged_df = pd.merge(all_user_df, trained_user_df, on='UserID', how='left', indicator=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "not_trained_user_df = merged_df.loc[merged_df['_merge'] == 'left_only', ['UserID', 'F', 'M']]\n",
        "not_trained_user_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_new_user_feat(graph, user_feature):\n",
        "    temple = graph.ndata['features']['user'].clone()\n",
        "    graph = dgl.add_nodes(graph, 1, ntype='user')\n",
        "    graph.nodes['user'].data['features'] = torch.cat((temple, user_feature.unsqueeze(0)), dim=0)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "u, v = validation_graph.edges(etype='rating')\n",
        "edges = list(zip(u.tolist(), v.tolist()))\n",
        "\n",
        "print(\"Các cạnh loại 'follows' trong đồ thị:\", len(edges))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_new_edge(graph, src_id, dest_id, edge_feature, etype):\n",
        "    graph.add_edges(src_id, dest_id, edge_feature, etype)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_user_id = trained_user_df['user_new_id'].max() if not trained_user_df.empty else 0\n",
        "for index, row in not_trained_user_df.iterrows():\n",
        "    user_id = row['UserID']\n",
        "    features = torch.tensor([row['F'], row['M']])  # Tạo tensor đặc trưng từ 'F' và 'M'\n",
        "\n",
        "    validation_graph = add_new_user_feat(validation_graph, features)\n",
        "\n",
        "    max_user_id += 1\n",
        "    trained_user_df.loc[len(trained_user_df)] = [user_id, max_user_id]\n",
        "\n",
        "validation_graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tìm các UserID chỉ có trong all_user_df mà không có trong trained_user_df\n",
        "merged_df = pd.merge(rating_train_df\n",
        "                     , trained_user_df, on='UserID', how='left', indicator=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('embeddings.pkl', 'wb') as f:\n",
        "    embeddings = get_embeddings(validation_graph, out_dim, trained_model, nodeLoad_test, num_batches_test, cuda, device, embedding_layer)\n",
        "    pickle.dump(embeddings, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_user_df.to_csv('user_id_table.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save lại graph mới (thêm user mới)\n",
        "dgl.save_graphs('graph_train.dgl', validation_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# add new edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_graph = validation_graph.clone()\n",
        "x_graph.num_edges('rating')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_graph.edges['bought-by'].data['rating'].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rating_star = 99\n",
        "x_graph = dgl.add_edges(x_graph, torch.tensor([3]), torch.tensor([3]),  data={'weight': torch.tensor(rating_star)}, etype='rating')\n",
        "x_graph = dgl.add_edges(x_graph, torch.tensor([3]), torch.tensor([3]), data={'weight': torch.tensor(rating_star)}, etype='bought-by')\n",
        "x_graph.num_edges('rating')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# recomend user có trong 6040 user với user_new_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recommend_for_user(user_id, validation_graph, embeddings, out_dim, k):\n",
        "    user_emb = embeddings['user'][user_id]\n",
        "    print(user_emb.size())\n",
        "    user_emb_rpt = user_emb.repeat(validation_graph.num_nodes('product'), 1)\n",
        "    \n",
        "    print(user_emb_rpt.size())\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    ratings = cos(user_emb_rpt, embeddings['product'])\n",
        "    \n",
        "    ratings_formatted = ratings.cpu().detach().numpy().reshape(validation_graph.num_nodes('product'),)\n",
        "    order = np.argsort(-ratings_formatted)\n",
        "    \n",
        "    rec = order[:k]  # top k recommendations\n",
        "    return rec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id_map = pd.read_csv('user_id_table.csv')\n",
        "product_id_map = pd.read_csv('product_id_table.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_embedded_and_recommend(user_id, validation_graph, trained_model, out_dim, device, k):\n",
        "    embeddings = get_embeddings(validation_graph, out_dim, trained_model, nodeLoad_test, num_batches_test, cuda, device, embedding_layer)\n",
        "    \n",
        "    recommended_products = recommend_for_user(user_id, validation_graph, embeddings, out_dim, k)\n",
        "    print(user_id, ' - ' , recommended_products)\n",
        "    return recommended_products\n",
        "\n",
        "# with open('./recommend api/config/embeddings.pkl', 'rb') as f:\n",
        "    #     embeddings = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getRealUserId(mappedId):\n",
        "    result = user_id_map.loc[user_id_map['user_new_id'] == mappedId, 'UserID']\n",
        "    \n",
        "    if not result.empty:\n",
        "        return result.iloc[0]\n",
        "    else:\n",
        "        return None \n",
        "\n",
        "def getMappedUserId(realId):\n",
        "    result = user_id_map.loc[user_id_map['UserID'] == realId, 'user_new_id']\n",
        "    \n",
        "    if not result.empty:\n",
        "        return result.iloc[0]\n",
        "    else:\n",
        "        return None \n",
        "    \n",
        "def getRealProductId(mappedId):\n",
        "    result = product_id_map.loc[product_id_map['product_new_id'] == mappedId, 'ItemID']\n",
        "    \n",
        "    if not result.empty:\n",
        "        return result.iloc[0]\n",
        "    else:\n",
        "        return None \n",
        "\n",
        "def getMappedProductId(realId):\n",
        "    result = product_id_map.loc[product_id_map['ItemID'] == realId, 'product_new_id']\n",
        "    \n",
        "    if not result.empty:\n",
        "        return result.iloc[0]\n",
        "    else:\n",
        "        return None \n",
        "    \n",
        "def getRealProductIds(recommended_products):\n",
        "    mapped_ids = []\n",
        "    for id in recommended_products:\n",
        "        mapped_id = getRealProductId(id)\n",
        "        if mapped_id is not None:\n",
        "            mapped_ids.append(mapped_id)\n",
        "    return mapped_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "recommended_products = calculate_embedded_and_recommend(132, validation_graph, trained_model, out_dim, device, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id_real = 'U00002222'\n",
        "user_id_mapped = getMappedUserId(user_id_real)\n",
        "recommended_products = calculate_embedded_and_recommend(user_id_mapped, validation_graph, trained_model, out_dim, device, k=10)\n",
        "mapped_ids = getRealProductIds(recommended_products)\n",
        "\n",
        "print(\"Các sản phẩm dành cho \",  user_id_real, \" là :\")\n",
        "print(mapped_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DL-Final-Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
